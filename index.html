<!DOCTYPE html><html><head><meta charset="utf-8"><title>Untitled Document.md</title><style></style></head><body id="preview">
<h1><a id="Practical_Machine_Learning_course_project_0"></a>Practical Machine Learning course project</h1>
<h2><a id="Prediction_Assignment_Writeup_2"></a>Prediction Assignment Writeup</h2>
<h3><a id="Introduction_4"></a>Introduction</h3>
<p>Using wearable devices it is now possible to collect data about personal activity. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, our goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: <a href="http://groupware.les.inf.puc-rio.br/har">http://groupware.les.inf.puc-rio.br/har</a> (see the section on the Weight Lifting Exercise Dataset).</p>
<h3><a id="Data_8"></a>Data</h3>
<p>The training data for this project are available here: <a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv">https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv</a></p>
<p>The test data are available here: <a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv">https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv</a></p>
<p>The data for this project come from this source: <a href="http://groupware.les.inf.puc-rio.br/har">http://groupware.les.inf.puc-rio.br/har</a>.</p>
<h3><a id="Goal_16"></a>Goal</h3>
<p>The goal of this project is to predict the manner in which people exercise. This is the “classe” variable in the training set. We  may use any of the variables in prediction. We will create a report describing how we built the  model, how we used cross validation, what is the out of sample error, and why we made some choices. We will  also use our model to  predict 20 different test cases.</p>
<h3><a id="Required_packages_20"></a>Required packages</h3>
<pre><code class="language-R"><span class="hljs-keyword">require</span>(caret)
<span class="hljs-keyword">require</span>(AppliedPredictiveModeling)
<span class="hljs-keyword">require</span>(rpart)
<span class="hljs-keyword">require</span>(randomForest)
<span class="hljs-keyword">require</span>(e1071)
<span class="hljs-keyword">require</span>(ggplot2)
<span class="hljs-keyword">require</span>(grid)
</code></pre>
<h3><a id="Loading_and_cleaning_the_training_data_32"></a>Loading and cleaning the training data</h3>
<pre><code class="language-R">rawTraining &lt;- read.csv(<span class="hljs-string">"pml-training.csv"</span>,na.strings = c(<span class="hljs-string">"NA"</span>, <span class="hljs-string">""</span>))
dim(rawTraining)
<span class="hljs-comment">## [1] 19622   160</span>
</code></pre>
<p>The training data set has 160 columns, some of which have mostly empty or NA entries. We select only variables with a valid first observation, which reduces the number of columns to 60. We check that there are no more NA entires.</p>
<pre><code class="language-R">tidyTraining &lt;- rawTraining[,!is.na(rawTraining[<span class="hljs-number">1</span>,])]
dim(tidyTraining)
<span class="hljs-comment">## [1] 19622    60</span>
sum(is.na(tidyTraining[,])); sum(complete.cases(tidyTraining))
<span class="hljs-comment">## [1] 0</span>
<span class="hljs-comment">## [1] 19622</span>
</code></pre>
<p>Eliminate columns: row id, time stamps, new_window, num_window</p>
<pre><code class="language-R">subTidyTraining &lt;- tidyTraining[,c(-<span class="hljs-number">1</span>,-<span class="hljs-number">3</span>,-<span class="hljs-number">4</span>,-<span class="hljs-number">5</span>,-<span class="hljs-number">6</span>,-<span class="hljs-number">7</span>)]
</code></pre>
<p>Investigate if there is an obvious correlation between user_name and classe.</p>
<pre><code class="language-R">subTidyTraining$user_name &lt;- as.integer(subTidyTraining$user_name)
<span class="hljs-keyword">attach</span>(mtcars)
par(mfrow=c(<span class="hljs-number">3</span>,<span class="hljs-number">2</span>))
hist(as.integer(subTidyTraining[subTidyTraining$user_name==<span class="hljs-number">1</span>,]$classe),main=<span class="hljs-string">"user 1"</span>,xlab=<span class="hljs-string">"classe"</span>,breaks=seq(<span class="hljs-number">0.5</span>,<span class="hljs-number">5.5</span>,l=<span class="hljs-number">6</span>))
hist(as.integer(subTidyTraining[subTidyTraining$user_name==<span class="hljs-number">2</span>,]$classe),main=<span class="hljs-string">"user 2"</span>,xlab=<span class="hljs-string">"classe"</span>,breaks=seq(<span class="hljs-number">0.5</span>,<span class="hljs-number">5.5</span>,l=<span class="hljs-number">6</span>))
hist(as.integer(subTidyTraining[subTidyTraining$user_name==<span class="hljs-number">3</span>,]$classe),main=<span class="hljs-string">"user 3"</span>,xlab=<span class="hljs-string">"classe"</span>,breaks=seq(<span class="hljs-number">0.5</span>,<span class="hljs-number">5.5</span>,l=<span class="hljs-number">6</span>))
hist(as.integer(subTidyTraining[subTidyTraining$user_name==<span class="hljs-number">4</span>,]$classe),main=<span class="hljs-string">"user 4"</span>,xlab=<span class="hljs-string">"classe"</span>,breaks=seq(<span class="hljs-number">0.5</span>,<span class="hljs-number">5.5</span>,l=<span class="hljs-number">6</span>))
hist(as.integer(subTidyTraining[subTidyTraining$user_name==<span class="hljs-number">5</span>,]$classe),main=<span class="hljs-string">"user 5"</span>,xlab=<span class="hljs-string">"classe"</span>,breaks=seq(<span class="hljs-number">0.5</span>,<span class="hljs-number">5.5</span>,l=<span class="hljs-number">6</span>))
hist(as.integer(subTidyTraining[subTidyTraining$user_name==<span class="hljs-number">6</span>,]$classe),main=<span class="hljs-string">"user 6"</span>,xlab=<span class="hljs-string">"classe"</span>,breaks=seq(<span class="hljs-number">0.5</span>,<span class="hljs-number">5.5</span>,l=<span class="hljs-number">6</span>))
par(mfrow=c(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))
</code></pre>
<p><img src="Images/Figure1UserHistogram.png" alt="Image of Histogram"></p>
<p>Remove the user_name variable since (i) the prediction should work for other people and (ii) there is
no strong correlation with classe. Check that “classe” is now in column 53.</p>
<pre><code class="language-R">subTidyTraining &lt;- subTidyTraining[,-<span class="hljs-number">1</span>]
names(subTidyTraining[<span class="hljs-number">53</span>])
<span class="hljs-comment">##[1] "classe"</span>
</code></pre>
<h3><a id="Preprocessing_77"></a>Preprocessing</h3>
<p>Check that there are no near zero variables. Other types of preprocessing have been tested by we skip them for the sake of  clarity in the final interpretation.</p>
<pre><code class="language-R">sum(nearZeroVar(subTidyTraining,saveMetrics = <span class="hljs-literal">TRUE</span>)$nzv)
<span class="hljs-comment">## [1] 0</span>
</code></pre>
<h3><a id="Split_the_Training_dataset_85"></a>Split the Training dataset</h3>
<p>Split training data into myTraining and myValidation subsets to estimate out-of-sample error.</p>
<pre><code class="language-R">inTrain &lt;- createDataPartition(subTidyTraining$classe, p=<span class="hljs-number">0.6</span>, list=<span class="hljs-literal">FALSE</span>)
myTraining &lt;- subTidyTraining[inTrain,]
myValidation &lt;- subTidyTraining[-inTrain,]
dim(myTraining); dim(myValidation)
<span class="hljs-comment">## [1] 11776    53</span>
<span class="hljs-comment">## [1] 7846   53</span>
</code></pre>
<h3><a id="Training_and_prediction_96"></a>Training and prediction</h3>
<p>Predicting with trees</p>
<pre><code class="language-R">set.seed(<span class="hljs-number">3433</span>)
modFitRpart &lt;- rpart(classe ~ ., data=myTraining, method=<span class="hljs-string">"class"</span>)
print(modFitRpart$finalModel)
predRpart &lt;- predict(modFitRpart,newdata = myValidation, type = <span class="hljs-string">"class"</span>)
confusionMatrix(myValidation$classe,predRpart)
<span class="hljs-comment">## Confusion Matrix and Statistics</span>
<span class="hljs-comment">## </span>
<span class="hljs-comment">##           Reference</span>
<span class="hljs-comment">## Prediction    A    B    C    D    E</span>
<span class="hljs-comment">##          A 1988   53   70   84   37</span>
<span class="hljs-comment">##          B  302  855  198  112   51</span>
<span class="hljs-comment">##          C   22  165 1077   92   12</span>
<span class="hljs-comment">##          D  120   66  188  855   57</span>
<span class="hljs-comment">##          E   65  131  188  137  921</span>
<span class="hljs-comment">## </span>
<span class="hljs-comment">## Overall Statistics</span>
<span class="hljs-comment">##                                          </span>
<span class="hljs-comment">##                Accuracy : 0.726          </span>
<span class="hljs-comment">##                  95% CI : (0.716, 0.7358)</span>
<span class="hljs-comment">##     No Information Rate : 0.3183         </span>
<span class="hljs-comment">##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16      </span>
<span class="hljs-comment">##                                          </span>
<span class="hljs-comment">##                   Kappa : 0.6522         </span>
<span class="hljs-comment">##  Mcnemar's Test P-Value : &lt; 2.2e-16      </span>
<span class="hljs-comment">## </span>
<span class="hljs-comment">## Statistics by Class:</span>
<span class="hljs-comment">## </span>
<span class="hljs-comment">##                      Class: A Class: B Class: C Class: D Class: E</span>
<span class="hljs-comment">## Sensitivity            0.7962   0.6732   0.6258   0.6680   0.8544</span>
<span class="hljs-comment">## Specificity            0.9544   0.8992   0.9525   0.9344   0.9230</span>
<span class="hljs-comment">## Pos Pred Value         0.8907   0.5632   0.7873   0.6649   0.6387</span>
<span class="hljs-comment">## Neg Pred Value         0.9093   0.9344   0.9006   0.9352   0.9755</span>
<span class="hljs-comment">## Prevalence             0.3183   0.1619   0.2193   0.1631   0.1374</span>
<span class="hljs-comment">## Detection Rate         0.2534   0.1090   0.1373   0.1090   0.1174</span>
<span class="hljs-comment">## Detection Prevalence   0.2845   0.1935   0.1744   0.1639   0.1838</span>
<span class="hljs-comment">## Balanced Accuracy      0.8753   0.7862   0.7891   0.8012   0.8887</span>
</code></pre>
<p>Predicting with support vector machines</p>
<pre><code class="language-R">set.seed(<span class="hljs-number">3435</span>)
modFitSVM &lt;- svm(classe~.,data=myTraining)
print(modFitSVM)
predSVM &lt;- predict(modFitSVM,newdata = myValidation)
confusionMatrix(myValidation$classe,predSVM)
<span class="hljs-comment">## Confusion Matrix and Statistics</span>
<span class="hljs-comment">## </span>
<span class="hljs-comment">##           Reference</span>
<span class="hljs-comment">## Prediction    A    B    C    D    E</span>
<span class="hljs-comment">##          A 2225    1    5    0    1</span>
<span class="hljs-comment">##          B  125 1365   28    0    0</span>
<span class="hljs-comment">##          C    1   36 1317   13    1</span>
<span class="hljs-comment">##          D    5    0  112 1166    3</span>
<span class="hljs-comment">##          E    0    8   31   48 1355</span>
<span class="hljs-comment">## </span>
<span class="hljs-comment">## Overall Statistics</span>
<span class="hljs-comment">##                                           </span>
<span class="hljs-comment">##                Accuracy : 0.9467          </span>
<span class="hljs-comment">##                  95% CI : (0.9415, 0.9516)</span>
<span class="hljs-comment">##     No Information Rate : 0.3003          </span>
<span class="hljs-comment">##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       </span>
<span class="hljs-comment">##                                           </span>
<span class="hljs-comment">##                   Kappa : 0.9325          </span>
<span class="hljs-comment">##  Mcnemar's Test P-Value : NA              </span>
<span class="hljs-comment">## </span>
<span class="hljs-comment">## Statistics by Class:</span>
<span class="hljs-comment">## </span>
<span class="hljs-comment">##                      Class: A Class: B Class: C Class: D Class: E</span>
<span class="hljs-comment">## Sensitivity            0.9444   0.9681   0.8821   0.9503   0.9963</span>
<span class="hljs-comment">## Specificity            0.9987   0.9762   0.9920   0.9819   0.9866</span>
<span class="hljs-comment">## Pos Pred Value         0.9969   0.8992   0.9627   0.9067   0.9397</span>
<span class="hljs-comment">## Neg Pred Value         0.9767   0.9929   0.9728   0.9907   0.9992</span>
<span class="hljs-comment">## Prevalence             0.3003   0.1797   0.1903   0.1564   0.1733</span>
<span class="hljs-comment">## Detection Rate         0.2836   0.1740   0.1679   0.1486   0.1727</span>
<span class="hljs-comment">## Detection Prevalence   0.2845   0.1935   0.1744   0.1639   0.1838</span>
<span class="hljs-comment">## Balanced Accuracy      0.9716   0.9722   0.9370   0.9661   0.9915</span>
</code></pre>
<p>Predicting with random forest</p>
<pre><code class="language-R">set.seed(<span class="hljs-number">3434</span>)
<span class="hljs-comment"># the caret version does not work well</span>
modFitRF &lt;- randomForest(classe~.,data=myTraining,ntree=<span class="hljs-number">400</span>,importance=<span class="hljs-literal">TRUE</span>)
predRF &lt;- predict(modFitRF,newdata = myValidation)
confusionMatrix(myValidation$classe,predRF)
<span class="hljs-comment">## Confusion Matrix and Statistics</span>
<span class="hljs-comment">## </span>
<span class="hljs-comment">##           Reference</span>
<span class="hljs-comment">## Prediction    A    B    C    D    E</span>
<span class="hljs-comment">##          A 2230    1    1    0    0</span>
<span class="hljs-comment">##          B   13 1504    1    0    0</span>
<span class="hljs-comment">##          C    0    6 1361    1    0</span>
<span class="hljs-comment">##          D    0    0   17 1267    2</span>
<span class="hljs-comment">##          E    0    0    2    7 1433</span>
<span class="hljs-comment">## </span>
<span class="hljs-comment">## Overall Statistics</span>
<span class="hljs-comment">##                                           </span>
<span class="hljs-comment">##                Accuracy : 0.9935          </span>
<span class="hljs-comment">##                  95% CI : (0.9915, 0.9952)</span>
<span class="hljs-comment">##     No Information Rate : 0.2859          </span>
<span class="hljs-comment">##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       </span>
<span class="hljs-comment">##                                           </span>
<span class="hljs-comment">##                   Kappa : 0.9918          </span>
<span class="hljs-comment">##  Mcnemar's Test P-Value : NA              </span>
<span class="hljs-comment">## </span>
<span class="hljs-comment">## Statistics by Class:</span>
<span class="hljs-comment">## </span>
<span class="hljs-comment">##                      Class: A Class: B Class: C Class: D Class: E</span>
<span class="hljs-comment">## Sensitivity            0.9942   0.9954   0.9848   0.9937   0.9986</span>
<span class="hljs-comment">## Specificity            0.9996   0.9978   0.9989   0.9971   0.9986</span>
<span class="hljs-comment">## Pos Pred Value         0.9991   0.9908   0.9949   0.9852   0.9938</span>
<span class="hljs-comment">## Neg Pred Value         0.9977   0.9989   0.9968   0.9988   0.9997</span>
<span class="hljs-comment">## Prevalence             0.2859   0.1926   0.1761   0.1625   0.1829</span>
<span class="hljs-comment">## Detection Rate         0.2842   0.1917   0.1735   0.1615   0.1826</span>
<span class="hljs-comment">## Detection Prevalence   0.2845   0.1935   0.1744   0.1639   0.1838</span>
<span class="hljs-comment">## Balanced Accuracy      0.9969   0.9966   0.9919   0.9954   0.9986</span>
</code></pre>
<h3><a id="Accuracy_and_out_of_sample_error_218"></a>Accuracy and out of sample error</h3>
<p>The accuracy is 73%, for the tree prediction, 93% for SVM, and 99% for random forest. RF is the winner and there is no point in increasing the number of trees, as shown in the pictures. We will focus on RF for in the variable analysis.</p>
<p>The out of sample error for RF is 1-0.9935=0.0065, or 0.65%.</p>
<pre><code class="language-R">plot(modFitRF,main=<span class="hljs-string">"No singificant improvement in RF fit for &gt;200 trees"</span>)
</code></pre>
<p><img src="Images/Figure2NumberOfTreesInRandomForest.png" alt="Image of RandomForect"></p>
<h3><a id="Variable_analysis_228"></a>Variable analysis</h3>
<p>Using the “importance” matrix from the RF prediciton in the analysis</p>
<pre><code class="language-R">importanceDF &lt;- data.frame(importance(modFitRF)[,c(<span class="hljs-number">6</span>,<span class="hljs-number">7</span>)])
importanceDF &lt;- cbind(rownames(importanceDF),importanceDF)
colnames(importanceDF)=c(<span class="hljs-string">"Variable"</span>,<span class="hljs-string">"MeanDecreaseAccuracy"</span>,<span class="hljs-string">"MeanDecreaseGini"</span>)
</code></pre>
<p>Order by accuracy and create a plot to identify “heavy hitters”</p>
<pre><code class="language-R">importanceDFaccuSort &lt;- transform(importanceDF, Variable = reorder(Variable, MeanDecreaseAccuracy))
accuracyPlot&lt;-ggplot(data=importanceDFaccuSort, aes(x=Variable, y=MeanDecreaseAccuracy)) + 
  ylab(<span class="hljs-string">"Mean Decrease Accuracy"</span>)+xlab(<span class="hljs-string">""</span>)+geom_bar(stat=<span class="hljs-string">"identity"</span>,width=<span class="hljs-number">.7</span>)+coord_flip() 
grid.draw(accuracyPlot)
</code></pre>
<p><img src="Images/Figure3VariablesAndAccuracy.png" alt="Image of Accuracy"></p>
<p>Order by Giniand create a plot to identify “heavy hitters”</p>
<pre><code class="language-R">importanceDFginiSort &lt;- transform(importanceDF, Variable = reorder(Variable, MeanDecreaseGini))
giniPlot=ggplot(data=importanceDFginiSort, aes(x=Variable, y=MeanDecreaseGini)) + 
  ylab(<span class="hljs-string">"Mean Decrease Gini"</span>)+xlab(<span class="hljs-string">""</span>)+geom_bar(stat=<span class="hljs-string">"identity"</span>,width=<span class="hljs-number">.7</span>)+ coord_flip() 
grid.draw(giniPlot)
</code></pre>
<p><img src="Images/Figure4VariablesAndGini.png" alt="Image of Gini"></p>
<h3><a id="Apply_prediction_to_the_test_data_set_256"></a>Apply prediction to the test data set</h3>
<pre><code class="language-R">rawTesting &lt;- read.csv(<span class="hljs-string">"pml-testing.csv"</span>,na.strings = c(<span class="hljs-string">"NA"</span>, <span class="hljs-string">""</span>))
</code></pre>
<p>Use the same subset of columns as in the training dataset (except the “classe” variable)</p>
<pre><code class="language-R">colSelect &lt;- colnames(subTidyTraining[,-<span class="hljs-number">53</span>])
subTidyTesting &lt;- rawTesting[colSelect]
</code></pre>
<p>predict using Random Forest</p>
<pre><code class="language-R">predRFtesting &lt;- predict(modFitRF,newdata = subTidyTesting)
predRFtesting
 <span class="hljs-comment">## 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 </span>
 <span class="hljs-comment">## B  A  B  A  A  E  D  B  A  A  B  C  B  A  E  E  A  B  B  B </span>
<span class="hljs-comment">## Levels: A B C D E</span>
</code></pre>

</body></html>